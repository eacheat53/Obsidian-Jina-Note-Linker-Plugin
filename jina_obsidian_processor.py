# -*- coding: utf-8 -*-
import os
import json
import re
import argparse
import requests
import math
import time
import datetime
import hashlib # ç”¨äº SHA256 å“ˆå¸Œè®¡ç®—
import fnmatch
import yaml # ç”¨äºè§£æ frontmatter
import sys
import io
from pathlib import Path # ç”¨äºè·¯å¾„æ“ä½œ

# ç¡®ä¿æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯ä½¿ç”¨ UTF-8 ç¼–ç 
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# --- åµŒå…¥å¤„ç†é…ç½®å¸¸é‡ (åªä¿ç•™è„šæœ¬å†…éƒ¨å›ºå®šæˆ–çœŸæ­£æ„ä¹‰ä¸Šçš„å¸¸é‡) ---
JINA_API_URL = "https://api.jina.ai/v1/embeddings" 
JINA_API_REQUEST_DELAY = 0.1 # Jina API è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
# DEFAULT_EMBEDDINGS_FILE_NAME, DEFAULT_CANDIDATES_FILE_NAME etc. are used as argparse defaults below

# --- AI æ‰“åˆ†ç›¸å…³é…ç½® ---
AI_API_REQUEST_DELAY_SECONDS = 1.0 # AI API è°ƒç”¨ä¹‹é—´çš„é»˜è®¤å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

# AI æä¾›å•†é»˜è®¤é…ç½®
DEFAULT_AI_CONFIGS = {
    'deepseek': {
        'api_url': 'https://api.deepseek.com/chat/completions',
        'model_name': 'deepseek-chat'
    },
    'openai': {
        'api_url': 'https://api.openai.com/v1/chat/completions',
        'model_name': 'gpt-4o-mini'
    },
    'claude': {
        'api_url': 'https://api.anthropic.com/v1/messages',
        'model_name': 'claude-3-haiku-20240307'
    },
    'gemini': {
        'api_url': 'https://generativelanguage.googleapis.com/v1beta/models',
        'model_name': 'gemini-1.5-flash'
    },
    'custom': {
        'api_url': '',
        'model_name': ''
    }
}



# --- å“ˆå¸Œè¾¹ç•Œæ ‡è®°å¸¸é‡ ---
# æ­¤æ ‡è®°ç”¨äºç•Œå®šç¬”è®°å†…å®¹å“ˆå¸Œè®¡ç®—çš„è¾¹ç•Œï¼Œæ­¤è¾¹ç•Œåçš„å†…å®¹ä¸å‚ä¸å“ˆå¸Œè®¡ç®—
HASH_BOUNDARY_MARKER = "<!-- HASH_BOUNDARY -->"

# --- è¾…åŠ©å‡½æ•° ---
def read_markdown_with_frontmatter(file_path: str) -> tuple[str, dict, str]:
    """
    è¯»å– Markdown æ–‡ä»¶ï¼Œåˆ†ç¦» frontmatterã€æ­£æ–‡å’ŒåŸå§‹ frontmatter å­—ç¬¦ä¸²ã€‚
    è¿”å› (æ­£æ–‡å†…å®¹, frontmatterå­—å…¸, åŸå§‹frontmatterå­—ç¬¦ä¸²)ã€‚
    å¦‚æœæ—  frontmatterï¼Œåˆ™ frontmatterå­—å…¸ ä¸ºç©ºï¼ŒåŸå§‹frontmatterå­—ç¬¦ä¸² ä¸ºç©ºã€‚
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        full_content = f.read()

    frontmatter_str = ""
    frontmatter_dict = {}
    body_content = full_content

    # æ£€æŸ¥æ˜¯å¦ä»¥frontmatterå¼€å¤´
    if full_content.startswith("---"):
        # æŸ¥æ‰¾frontmatterç»“æŸæ ‡è®°
        lines = full_content.split('\n')
        frontmatter_end_line = -1
        
        # ä»ç¬¬äºŒè¡Œå¼€å§‹æŸ¥æ‰¾ç»“æŸçš„ ---
        for i in range(1, len(lines)):
            if lines[i].strip() == "---":
                frontmatter_end_line = i
                break
        
        if frontmatter_end_line != -1:
            # æå–frontmatterå†…å®¹ï¼ˆä¸åŒ…æ‹¬å¼€å§‹å’Œç»“æŸçš„ --- è¡Œï¼‰
            frontmatter_lines = lines[1:frontmatter_end_line]
            frontmatter_block = '\n'.join(frontmatter_lines)
            
            # æå–bodyå†…å®¹ï¼ˆä»frontmatterç»“æŸè¡Œçš„ä¸‹ä¸€è¡Œå¼€å§‹ï¼‰
            body_lines = lines[frontmatter_end_line + 1:]
            body_content = '\n'.join(body_lines)
            
            frontmatter_str = frontmatter_block 
            try:
                frontmatter_dict = yaml.safe_load(frontmatter_block) or {}
            except yaml.YAMLError as e:
                print(f"è­¦å‘Šï¼šè§£ææ–‡ä»¶ {file_path} çš„ frontmatter å¤±è´¥: {e}")
                frontmatter_dict = {} 
    
    return body_content, frontmatter_dict, frontmatter_str


def write_markdown_with_frontmatter(file_path: str, frontmatter: dict, body: str):
    """
    å°† frontmatter å’Œæ­£æ–‡é‡æ–°ç»„åˆå¹¶å†™å…¥ Markdown æ–‡ä»¶ã€‚
    """
    output_content = ""
    if frontmatter:
        frontmatter_dump = yaml.dump(frontmatter, allow_unicode=True, default_flow_style=False, sort_keys=False)
        output_content = f"---\n{frontmatter_dump.strip()}\n---\n"
    
    output_content += body

    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(output_content)

def extract_content_for_hashing(text_body: str) -> str | None:
    """
    æå–ç”¨äºè®¡ç®—å“ˆå¸Œçš„å†…å®¹ã€‚
    å¿…é¡»æ‰¾åˆ° HASH_BOUNDARY_MARKERï¼Œå¹¶ä½¿ç”¨æ ‡è®°ä¹‹å‰çš„å†…å®¹ã€‚
    å¦‚æœæœªæ‰¾åˆ° HASH_BOUNDARY_MARKERï¼Œåˆ™è¿”å› Noneï¼Œè¡¨ç¤ºæ— æ³•æå–å“ˆå¸Œå†…å®¹ã€‚
    æå–å‡ºçš„å†…å®¹ä¼šè¿›è¡Œæœ«å°¾æ¢è¡Œç¬¦çš„æ ‡å‡†åŒ–å¤„ç†ã€‚
    """
    marker_index = text_body.find(HASH_BOUNDARY_MARKER)

    if marker_index != -1:
        content_to_hash = text_body[:marker_index]
        if not content_to_hash.strip(): 
            return "" 
        content_to_hash = content_to_hash.rstrip('\r\n') 
        return content_to_hash + "\n" 
    else:
        return None

def calculate_hash_from_content(content: str) -> str:
    """è®¡ç®—ç»™å®šå­—ç¬¦ä¸²å†…å®¹çš„ SHA256 å“ˆå¸Œå€¼ã€‚"""
    hasher = hashlib.sha256()
    hasher.update(content.encode('utf-8'))
    return hasher.hexdigest()

def list_markdown_files(scan_directory_abs: str, project_root_abs: str, excluded_folders: list = None, excluded_files_patterns: list = None) -> list:
    markdown_files = []
    if excluded_folders is None:
        excluded_folders = []
    if excluded_files_patterns is None:
        excluded_files_patterns = []

    if not os.path.isdir(scan_directory_abs):
        print(f"é”™è¯¯ï¼šæ‰«æè·¯å¾„ {scan_directory_abs} ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
        return []

    compiled_excluded_patterns = []
    for p_glob in excluded_files_patterns:
        try:
            p_regex = fnmatch.translate(p_glob) 
            compiled_excluded_patterns.append(re.compile(p_regex, re.IGNORECASE))
        except re.error as e:
            print(f"è­¦å‘Šï¼šæ— æ³•å°†æ’é™¤æ–‡ä»¶ Glob æ¨¡å¼ '{p_glob}' è½¬æ¢ä¸ºæœ‰æ•ˆæ­£åˆ™è¡¨è¾¾å¼: {e}ã€‚å°†è·³è¿‡æ­¤æ¨¡å¼ã€‚")

    for root, dirs, files in os.walk(scan_directory_abs, topdown=True):
        dirs[:] = [d for d in dirs if d.lower() not in [ef.lower() for ef in excluded_folders]]
        
        for file in files:
            if file.endswith(".md"):
                file_abs_path = os.path.join(root, file)
                is_excluded_file = False
                for pattern in compiled_excluded_patterns:
                    if pattern.search(file.lower()) or pattern.search(os.path.splitext(file)[0].lower()):
                        is_excluded_file = True
                        break
                if is_excluded_file:
                    continue
                
                path_relative_to_project_root = os.path.relpath(file_abs_path, project_root_abs)
                path_parts = path_relative_to_project_root.lower().split(os.sep)
                if any(part in [ef.lower() for ef in excluded_folders] for part in path_parts[:-1]):
                    continue

                markdown_files.append(path_relative_to_project_root.replace(os.sep, '/'))
    return markdown_files

def get_jina_embedding(text: str, 
                       jina_api_key_to_use: str, 
                       jina_model_name_to_use: str, 
                       max_retries: int = 3, 
                       initial_delay: float = 1.0) -> list | None:
    """è°ƒç”¨ Jina API è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
    if not jina_api_key_to_use: 
        print("é”™è¯¯ï¼šJina API Key æœªæä¾›ã€‚")
        return None
    if not jina_model_name_to_use:
        print("é”™è¯¯ï¼šJina æ¨¡å‹åç§°æœªæä¾›ã€‚")
        return None
    if not text.strip():
        print("è­¦å‘Šï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡åµŒå…¥ã€‚")
        return None

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {jina_api_key_to_use}"
    }
    data = {
        "input": [text],
        "model": jina_model_name_to_use
    }
    
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            time.sleep(JINA_API_REQUEST_DELAY)
            response = requests.post(JINA_API_URL, headers=headers, data=json.dumps(data), timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get("data") and len(result["data"]) > 0 and result["data"][0].get("embedding"):
                return result["data"][0]["embedding"]
            else:
                print(f"é”™è¯¯ï¼šJina API å“åº”æ ¼å¼ä¸æ­£ç¡®ã€‚å“åº”: {result}")
                return None # æ ¼å¼é”™è¯¯ï¼Œä¸é‡è¯•

        except requests.exceptions.RequestException as e:
            print(f"é”™è¯¯ï¼šè°ƒç”¨ Jina API å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"å“åº”çŠ¶æ€ç : {e.response.status_code}, å“åº”å†…å®¹: {e.response.text[:500]}...")
                # å¦‚æœæ˜¯å®¢æˆ·ç«¯é”™è¯¯ï¼ˆå¦‚4xxï¼‰ï¼Œåˆ™ä¸é‡è¯•
                if 400 <= e.response.status_code < 500:
                    return None
            
            # ç­‰å¾…åé‡è¯•
            time.sleep(delay)
            delay *= 2 # æŒ‡æ•°é€€é¿

        except Exception as e:
            print(f"å¤„ç† Jina API å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return None # æœªçŸ¥é”™è¯¯ï¼Œä¸é‡è¯•
            
    print(f"é”™è¯¯ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries} åï¼ŒJina API è°ƒç”¨ä»ç„¶å¤±è´¥ã€‚")
    return None

class EmbeddingEncoder(json.JSONEncoder):
    def iterencode(self, obj, _one_shot=False):
        if isinstance(obj, dict):
            for chunk in super().iterencode(obj, _one_shot):
                yield chunk
        elif isinstance(obj, list) and len(obj) > 0 and isinstance(obj[0], (int, float)):
            if len(obj) > 50: 
                yield '['
                yield ', '.join(f"{x:.8f}" for x in obj) 
                yield ']'
            else:
                for chunk in super().iterencode(obj, _one_shot):
                    yield chunk
        else:
            for chunk in super().iterencode(obj, _one_shot):
                yield chunk

def process_and_embed_notes(
    project_root_abs: str,
    files_relative_to_project_root: list,
    embeddings_file_path: str,
    # --- æ·»åŠ çš„å‚æ•° ---
    jina_api_key_to_use: str,
    jina_model_name_to_use: str,
    max_chars_for_jina_to_use: int
) -> dict:
    loaded_json_structure = {}
    files_data_from_json = {}
    metadata_from_json = {}  

    if os.path.exists(embeddings_file_path):
        try:
            with open(embeddings_file_path, 'r', encoding='utf-8') as f:
                loaded_json_structure = json.load(f)
                if isinstance(loaded_json_structure, dict):
                    files_data_from_json = loaded_json_structure.get("files", {})
                    metadata_from_json = loaded_json_structure.get("_metadata", {})
                    if not isinstance(files_data_from_json, dict):
                        files_data_from_json = {}
                    if not isinstance(metadata_from_json, dict):
                        metadata_from_json = {}
                else:
                    print(f"è­¦å‘Šï¼šJSONæ–‡ä»¶ {embeddings_file_path} é¡¶å±‚ä¸æ˜¯å­—å…¸ã€‚")
        except Exception as e:
            print(f"è­¦å‘Šï¼šåŠ è½½åµŒå…¥æ•°æ®æ–‡ä»¶ {embeddings_file_path} å¤±è´¥: {e}ã€‚")

    embedded_count = 0
    processed_files_this_run = 0
    
    for i, file_rel_path in enumerate(files_relative_to_project_root):
        file_abs_path = os.path.join(project_root_abs, file_rel_path)
        print(f"å¤„ç†æ–‡ä»¶ ({i+1}/{len(files_relative_to_project_root)}): {file_rel_path}")
        
        if not os.path.exists(file_abs_path):
            print(f"  é”™è¯¯ï¼šæ–‡ä»¶ {file_rel_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            if file_rel_path in files_data_from_json:
                del files_data_from_json[file_rel_path]
            continue

        original_body_content_from_read_fn = ""
        existing_frontmatter = {}
        try:
            original_body_content_from_read_fn, existing_frontmatter, _ = read_markdown_with_frontmatter(file_abs_path)
        except Exception as e:
            print(f"  é”™è¯¯ï¼šè¯»å–æ–‡ä»¶ {file_rel_path} å¤±è´¥: {e}ã€‚è·³è¿‡ã€‚")
            if file_rel_path in files_data_from_json:
                del files_data_from_json[file_rel_path]
            continue
        
        text_for_processing = extract_content_for_hashing(original_body_content_from_read_fn)
        
        if text_for_processing is None:
            print(f"  é”™è¯¯: ç¬”è®° '{file_rel_path}' ä¸­æœªæ‰¾åˆ°å“ˆå¸Œè¾¹ç•Œæ ‡è®° '{HASH_BOUNDARY_MARKER}'ã€‚è·³è¿‡ã€‚")
            if file_rel_path in files_data_from_json:
                del files_data_from_json[file_rel_path]
            continue

        current_content_hash = calculate_hash_from_content(text_for_processing)
        stored_hash_in_frontmatter = existing_frontmatter.get("jina_hash")

        needs_embedding_api_call = True
        final_embedding_for_json = None

        if stored_hash_in_frontmatter and stored_hash_in_frontmatter == current_content_hash:
            json_entry = files_data_from_json.get(file_rel_path)
            if json_entry and isinstance(json_entry, dict) and json_entry.get("hash") == current_content_hash:
                if json_entry.get("embedding") is not None:
                    final_embedding_for_json = json_entry["embedding"]
                    needs_embedding_api_call = False
                else: 
                    final_embedding_for_json = None
                    needs_embedding_api_call = False
        elif stored_hash_in_frontmatter:
             print(f"  å†…å®¹å·²ä¿®æ”¹ (frontmatterå“ˆå¸Œ '{stored_hash_in_frontmatter[:8]}' vs å½“å‰ \'{current_content_hash[:8]}\')ã€‚")
        else:
            print(f"  æ–°æ–‡ä»¶æˆ–frontmatterä¸­æ— å“ˆå¸Œã€‚")

        if needs_embedding_api_call:
            processed_files_this_run += 1
            if not text_for_processing.strip():
                print(f"  è­¦å‘Šï¼šæ–‡ä»¶ {file_rel_path} æœ‰æ•ˆå†…å®¹ä¸ºç©ºã€‚ä¸åµŒå…¥ã€‚")
                final_embedding_for_json = None
            else:
                if len(text_for_processing) > max_chars_for_jina_to_use:
                    print(f"  è­¦å‘Šï¼šæ–‡ä»¶ {file_rel_path} å†…å®¹è¿‡é•¿ ({len(text_for_processing)} chars)ï¼Œæˆªæ–­è‡³ {max_chars_for_jina_to_use} è¿›è¡ŒåµŒå…¥ã€‚")
                    text_for_processing_truncated = text_for_processing[:max_chars_for_jina_to_use]
                else:
                    text_for_processing_truncated = text_for_processing

                embedding_from_api = get_jina_embedding(text_for_processing_truncated, jina_api_key_to_use, jina_model_name_to_use)
                if embedding_from_api:
                    final_embedding_for_json = embedding_from_api
                    embedded_count += 1
                    print(f"  æˆåŠŸè·å–åµŒå…¥: {file_rel_path} (å“ˆå¸Œ: {current_content_hash[:8]}...)")
                else:
                    final_embedding_for_json = None
                    print(f"  æœªèƒ½è·å–åµŒå…¥: {file_rel_path}")
        
        existing_frontmatter["jina_hash"] = current_content_hash
        try:
            write_markdown_with_frontmatter(file_abs_path, existing_frontmatter, original_body_content_from_read_fn)
        except Exception as e_write:
            print(f"  é”™è¯¯ï¼šå†™å…¥ frontmatter åˆ° {file_rel_path} å¤±è´¥: {e_write}")

        files_data_from_json[file_rel_path] = {
            "embedding": final_embedding_for_json,
            "hash": current_content_hash,
            "processed_content": text_for_processing  # ä¿å­˜å·²å¤„ç†çš„å†…å®¹ä¾›AIè¯„åˆ†ä½¿ç”¨
        }

    final_metadata = {
        "generated_at_utc": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "jina_model_name": jina_model_name_to_use, # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°
        "script_version": "2.0_plugin_compatible" 
    }
    
    output_for_return_and_saving = {
        "_metadata": final_metadata,
        "files": files_data_from_json
    }
    
    needs_saving = os.path.exists(embeddings_file_path) or processed_files_this_run > 0

    if needs_saving:
        try:
            with open(embeddings_file_path, 'w', encoding='utf-8') as f:
                json.dump(output_for_return_and_saving, f, ensure_ascii=False, indent=4, cls=EmbeddingEncoder)
            print(f"åµŒå…¥æ•°æ®å·²ä¿å­˜åˆ° {embeddings_file_path}ã€‚æœ¬æ¬¡åµŒå…¥/æ›´æ–° {embedded_count} æ¡ã€‚å¤„ç†äº† {processed_files_this_run} ä¸ªæ–‡ä»¶ã€‚")
        except Exception as e:
            print(f"é”™è¯¯ï¼šä¿å­˜åµŒå…¥æ•°æ®åˆ° {embeddings_file_path} å¤±è´¥: {e}")
            
    return output_for_return_and_saving

def cosine_similarity(vec1: list, vec2: list) -> float:
    if not vec1 or not vec2 or len(vec1) != len(vec2):
        return 0.0
    dot_product = sum(p * q for p, q in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(p * p for p in vec1))
    magnitude2 = math.sqrt(sum(q * q for q in vec2))
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    return dot_product / (magnitude1 * magnitude2)

def generate_candidate_pairs(embeddings_data_input: dict, similarity_threshold: float) -> list: # candidates_file_path å·²ç§»é™¤
    actual_embeddings_data = {}
    if isinstance(embeddings_data_input, dict) and "files" in embeddings_data_input: # æ£€æŸ¥æ–°ç»“æ„
        actual_embeddings_data = embeddings_data_input["files"]
    elif isinstance(embeddings_data_input, dict): # å…¼å®¹æ—§ç»“æ„
        actual_embeddings_data = {k:v for k,v in embeddings_data_input.items() if k != "_metadata"}
    else:
        print("é”™è¯¯ï¼šä¼ å…¥ generate_candidate_pairs çš„ embeddings_data_input æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return []

    processed_embeddings = {}
    for path, data_entry in actual_embeddings_data.items():
        if data_entry and isinstance(data_entry, dict) and \
           "embedding" in data_entry and data_entry["embedding"] and isinstance(data_entry["embedding"], list):
            processed_embeddings[path] = data_entry["embedding"]
        
    file_paths = list(processed_embeddings.keys())
    if len(file_paths) < 2:
        print("æœ‰æ•ˆåµŒå…¥å°‘äº2ä¸ªï¼Œæ— æ³•ç”Ÿæˆå€™é€‰å¯¹ã€‚")
        return []
    
    candidate_pairs = []
    print(f"å¼€å§‹ä» {len(file_paths)} ä¸ªæœ‰æ•ˆåµŒå…¥ç¬”è®°ä¸­å…¨æ–°ç”Ÿæˆå€™é€‰å¯¹...")
    total_comparisons = len(file_paths) * (len(file_paths) - 1) // 2
    
    processed_pairs_count = 0
    for i in range(len(file_paths)):
        for j in range(i + 1, len(file_paths)):
            path1, path2 = file_paths[i], file_paths[j]
            emb1, emb2 = processed_embeddings[path1], processed_embeddings[path2]
            similarity = cosine_similarity(emb1, emb2)
            processed_pairs_count += 1
            if processed_pairs_count % 5000 == 0 or processed_pairs_count == total_comparisons:
                print(f"  å·²æ¯”è¾ƒ {processed_pairs_count}/{total_comparisons} å¯¹ç¬”è®°...")

            if similarity >= similarity_threshold:
                # ç”ŸæˆåŒå‘å…³ç³»ï¼Œä½†é¿å…åœ¨AIæ‰“åˆ†é˜¶æ®µé‡å¤å¤„ç†
                candidate_pairs.append({
                    "source_path": path1, 
                    "target_path": path2, 
                    "jina_similarity": similarity,
                    "pair_id": f"{min(path1, path2)}<->{max(path1, path2)}"  # å”¯ä¸€æ ‡è¯†ç¬¦
                })
                candidate_pairs.append({
                    "source_path": path2, 
                    "target_path": path1, 
                    "jina_similarity": similarity,
                    "pair_id": f"{min(path1, path2)}<->{max(path1, path2)}"  # ç›¸åŒçš„æ ‡è¯†ç¬¦
                })
    
    final_map = {}
    for p in candidate_pairs:
        src_path_norm = p["source_path"].replace(os.sep, '/')
        tgt_path_norm = p["target_path"].replace(os.sep, '/')
        key = (src_path_norm, tgt_path_norm, round(p["jina_similarity"], 6))
        if key not in final_map:
            p_norm = p.copy()
            p_norm["source_path"] = src_path_norm
            p_norm["target_path"] = tgt_path_norm
            final_map[key] = p_norm
    
    unique_sorted_pairs = sorted(
        list(final_map.values()),
        key=lambda x: (x["source_path"], -x["jina_similarity"], x["target_path"])
    )
    print(f"æœ€ç»ˆç”Ÿæˆ {len(unique_sorted_pairs)} ä¸ªå€™é€‰é“¾æ¥å¯¹ã€‚")
    return unique_sorted_pairs

# REMOVED: get_deepseek_api_key function. Key is passed directly.

def call_ai_api_for_pair_relevance(
    source_processed_content: str,  # å·²ç»å¤„ç†è¿‡çš„å†…å®¹ï¼Œä¸éœ€è¦å†æ¬¡æå–
    target_processed_content: str,  # å·²ç»å¤„ç†è¿‡çš„å†…å®¹ï¼Œä¸éœ€è¦å†æ¬¡æå–
    source_file_path: str,
    target_file_path: str, 
    api_key: str,
    # --- AI æä¾›å•†å‚æ•° ---
    ai_provider: str,
    ai_api_url: str,
    ai_model_name: str,
    max_content_length_for_ai_to_use: int,
    max_retries: int = 3,
    initial_delay: float = 1.0
) -> dict:
    if not api_key:
        print(f"é”™è¯¯ï¼š{ai_provider} API Key æœªé…ç½®ã€‚æ— æ³•ä¸º {source_file_path} -> {target_file_path} æ‰“åˆ†ã€‚")
        return {"ai_score": -1, "error": "API Key not configured"}

    source_file_name = os.path.basename(source_file_path)
    target_file_name = os.path.basename(target_file_path)

    # ç›´æ¥ä½¿ç”¨å·²ç»å¤„ç†è¿‡çš„å†…å®¹ï¼Œä¸éœ€è¦å†æ¬¡æ£€æŸ¥å“ˆå¸Œè¾¹ç•Œæ ‡è®°
    source_excerpt = source_processed_content[:max_content_length_for_ai_to_use]
    target_excerpt = target_processed_content[:max_content_length_for_ai_to_use]

    # æ„å»ºè¯·æ±‚ä½“å’Œå¤´éƒ¨ï¼Œæ ¹æ®ä¸åŒAIæä¾›å•†è°ƒæ•´
    request_body, headers = build_ai_request(
        ai_provider, ai_model_name, api_key, source_file_name, target_file_name,
        source_excerpt, target_excerpt, max_content_length_for_ai_to_use
    )

    print(f"  AIæ‰“åˆ† (è°ƒç”¨{ai_provider} API): {source_file_path} -> {target_file_path}")
    
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            # å¯¹äºGeminiï¼Œéœ€è¦åœ¨URLä¸­æ·»åŠ APIå¯†é’¥
            if ai_provider == 'gemini':
                # æ„å»ºå®Œæ•´çš„Gemini API URL
                model_path = ai_model_name if ai_model_name else 'gemini-1.5-flash'
                full_url = f"{ai_api_url}/{model_path}:generateContent?key={api_key}"
            else:
                full_url = ai_api_url
                
            # å»¶è¿Ÿç”±è°ƒç”¨è€…å¤„ç† (score_candidates_and_update_frontmatter)
            response = requests.post(full_url, headers=headers, json=request_body, timeout=45)
            
            if not response.ok:
                error_message = f"{ai_provider} API å¤±è´¥ {source_file_path}->{target_file_path}: HTTP {response.status_code}"
                try: error_message += f" - {response.json()}"
                except json.JSONDecodeError: error_message += f" - {response.text[:200]}"
                print(error_message)
                # å¦‚æœæ˜¯å®¢æˆ·ç«¯é”™è¯¯ï¼ˆå¦‚4xxï¼‰ï¼Œåˆ™ä¸é‡è¯•
                if 400 <= response.status_code < 500:
                    return {"ai_score": -1, "error": f"API Error: HTTP {response.status_code}"}
                # å¯¹äºæœåŠ¡å™¨é”™è¯¯ï¼Œè¿›è¡Œé‡è¯•
                raise requests.exceptions.RequestException(f"Server error: {response.status_code}")

            # è§£æå“åº”ï¼Œæ ¹æ®ä¸åŒAIæä¾›å•†è°ƒæ•´
            score = parse_ai_response(response, ai_provider, source_file_path, target_file_path)
            if score is not None:
                return {"ai_score": score}
            else:
                return {"ai_score": -1, "error": "Failed to parse AI response"} # è§£æå¤±è´¥ï¼Œä¸é‡è¯•
                
        except requests.exceptions.Timeout:
            print(f"{ai_provider} API è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}) {source_file_path}->{target_file_path}")
            time.sleep(delay)
            delay *= 2
        except requests.exceptions.RequestException as e:
            print(f"{ai_provider} API è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            time.sleep(delay)
            delay *= 2
        except Exception as e_unknown: # æ•è·æ›´å¹¿æ³›çš„å¼‚å¸¸
            print(f"{ai_provider} API æœªçŸ¥é”™è¯¯ {source_file_path}->{target_file_path}: {e_unknown}")
            return {"ai_score": -1, "error": f"Unknown API call error: {e_unknown}"} # æœªçŸ¥é”™è¯¯ï¼Œä¸é‡è¯•
            
    print(f"é”™è¯¯ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries} åï¼ŒAI API è°ƒç”¨ä»ç„¶å¤±è´¥ã€‚")
    return {"ai_score": -1, "error": "API call failed after multiple retries"}


def build_ai_request(ai_provider: str, model_name: str, api_key: str, 
                    source_file_name: str, target_file_name: str,
                    source_excerpt: str, target_excerpt: str, 
                    max_content_length: int) -> tuple[dict, dict]:
    """æ„å»ºä¸åŒAIæä¾›å•†çš„è¯·æ±‚ä½“å’Œå¤´éƒ¨"""
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ª Obsidian ç¬”è®°é“¾æ¥è¯„ä¼°åŠ©æ‰‹ã€‚è¯·ç›´æ¥æ¯”è¾ƒä»¥ä¸‹ã€æºç¬”è®°å†…å®¹ã€‘å’Œã€ç›®æ ‡ç¬”è®°å†…å®¹ã€‘ï¼Œåˆ¤æ–­å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
ä½ çš„ä»»åŠ¡æ˜¯è¯„ä¼°ä»æºç¬”è®°æŒ‡å‘ç›®æ ‡ç¬”è®°å»ºç«‹ä¸€ä¸ªé“¾æ¥æ˜¯å¦åˆé€‚ã€‚
è¯·ç»™å‡º 0-10 ä¹‹é—´çš„æ•´æ•°è¯„åˆ†ï¼Œå…¶ä¸­ 10 è¡¨ç¤ºæå…¶ç›¸å…³ï¼Œ7-9 è¡¨ç¤ºæ¯”è¾ƒç›¸å…³ï¼Œ6 è¡¨ç¤ºä½ è®¤ä¸ºåˆæ ¼çš„ç›¸å…³æ€§ï¼Œ1-5 è¡¨ç¤ºå¼±ç›¸å…³ï¼Œ0 è¡¨ç¤ºä¸ç›¸å…³æˆ–æ— æ³•åˆ¤æ–­ã€‚

æºç¬”è®°æ–‡ä»¶å: {source_file_name}
ç›®æ ‡ç¬”è®°æ–‡ä»¶å: {target_file_name}

ã€æºç¬”è®°å†…å®¹ã€‘(æœ€å¤š {max_content_length} å­—ç¬¦):
---
{source_excerpt}
---

ã€ç›®æ ‡ç¬”è®°å†…å®¹ã€‘(æœ€å¤š {max_content_length} å­—ç¬¦):
---
{target_excerpt}
---

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ä½ çš„è¯„åˆ†: {{"relevance_score": <ä½ çš„è¯„åˆ†>}}
ä¾‹å¦‚: {{"relevance_score": 8}}
è¿”å›çº¯ç²¹çš„JSONï¼Œä¸åŒ…å«ä»»ä½•Markdownæ ‡è®°ã€‚"""

    if ai_provider == 'claude':
        # Claude API æ ¼å¼
        request_body = {
            "model": model_name,
            "max_tokens": 1024,
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {
            'Content-Type': 'application/json',
            'x-api-key': api_key,
            'anthropic-version': '2023-06-01'
        }
    elif ai_provider == 'gemini':
        # Gemini API æ ¼å¼
        request_body = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "response_mime_type": "application/json"
            }
        }
        headers = {
            'Content-Type': 'application/json'
        }
    else:
        # OpenAI å…¼å®¹æ ¼å¼ (DeepSeek, OpenAI, Custom)
        request_body = {
            "model": model_name,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False
        }
        if ai_provider in ['deepseek', 'openai']:
            request_body["response_format"] = {"type": "json_object"}
        
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}'
        }
    
    return request_body, headers


def parse_ai_response(response: requests.Response, ai_provider: str, 
                     source_file_path: str, target_file_path: str) -> int | None:
    """è§£æä¸åŒAIæä¾›å•†çš„å“åº”"""
    try:
        data = response.json()
        
        if ai_provider == 'claude':
            # Claude å“åº”æ ¼å¼
            if data and data.get("content") and len(data["content"]) > 0:
                message_content_str = data["content"][0].get("text", "")
            else:
                print(f"Claude API å“åº”æ ¼å¼ä¸ç¬¦ {source_file_path}->{target_file_path}: {data}")
                return None
        elif ai_provider == 'gemini':
            # Gemini å“åº”æ ¼å¼
            if data and data.get("candidates") and len(data["candidates"]) > 0:
                content = data["candidates"][0].get("content", {})
                if content.get("parts") and len(content["parts"]) > 0:
                    message_content_str = content["parts"][0].get("text", "")
                else:
                    print(f"Gemini API å“åº”æ ¼å¼ä¸ç¬¦ {source_file_path}->{target_file_path}: {data}")
                    return None
            else:
                print(f"Gemini API å“åº”æ ¼å¼ä¸ç¬¦ {source_file_path}->{target_file_path}: {data}")
                return None
        else:
            # OpenAI å…¼å®¹æ ¼å¼
            if data and data.get("choices") and data["choices"][0].get("message", {}).get("content"):
                message_content_str = data["choices"][0]["message"]["content"]
            else:
                print(f"{ai_provider} API å“åº”æ ¼å¼ä¸ç¬¦ {source_file_path}->{target_file_path}: {data}")
                return None
        
        # è§£æJSONè¯„åˆ†
        try:
            cleaned_json_str = re.sub(r'^```json\s*|\s*```$', '', message_content_str.strip(), flags=re.DOTALL)
            parsed_json = json.loads(cleaned_json_str)
            if isinstance(parsed_json.get("relevance_score"), (int, float)):
                score = max(0, min(10, round(float(parsed_json["relevance_score"]))))
                return score
            else:
                print(f"{ai_provider} API JSON ç»“æ„ä¸å®Œæ•´ {source_file_path}->{target_file_path}: {parsed_json}")
                return None
        except json.JSONDecodeError as e_json:
            print(f"{ai_provider} API å“åº”éJSON {source_file_path}->{target_file_path}: '{message_content_str}', Error: {e_json}")
            return None
            
    except json.JSONDecodeError as e:
        print(f"{ai_provider} API å“åº”è§£æå¤±è´¥ {source_file_path}->{target_file_path}: {e}")
        return None


def normalize_path_python(path_str: str) -> str:
    if not path_str: return ""
    return path_str.replace(os.sep, '/')

def score_candidates_and_update_frontmatter(
    candidate_pairs_list: list,
    project_root_abs: str,
    # --- AI provider parameters ---
    ai_provider: str,
    ai_api_url: str,
    ai_api_key: str,
    ai_model_name: str,
    # --- Other parameters ---
    max_content_length_for_ai_to_use: int,
    max_candidates_per_source_for_ai_scoring_to_use: int,
    hash_boundary_marker_to_use: str,
    force_rescore: bool
):
    """
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åˆ é™¤AIè¯„åˆ†å†™å…¥YAML frontmatteråŠŸèƒ½
    ç°åœ¨åªè¿›è¡ŒAIè¯„åˆ†å¹¶ä¿å­˜åˆ°ç‹¬ç«‹JSONæ–‡ä»¶
    """
    if not ai_api_key: # This check is now primary
        print(f"é”™è¯¯ï¼š{ai_provider} API Key æœªæä¾›ï¼Œè·³è¿‡ AI æ‰“åˆ†æµç¨‹ã€‚")
        return

    updated_files_count = 0
    
    # ğŸ”¥ æ–°å¢ï¼šAIæ‰“åˆ†å»é‡é€»è¾‘
    print("æ­£åœ¨å¯¹å€™é€‰å¯¹è¿›è¡Œå»é‡ä»¥é¿å…é‡å¤AIæ‰“åˆ†...")
    unique_pairs_for_ai = {}  # å­˜å‚¨å”¯ä¸€çš„å…³ç³»å¯¹ï¼Œç”¨äºAIæ‰“åˆ†
    ai_score_cache = {}       # ç¼“å­˜AIè¯„åˆ†ç»“æœ
    
    # ğŸ”¥ æ–°å¢ï¼šåŠ è½½å·²æœ‰çš„AIè¯„åˆ†æ•°æ®
    ai_scores_file_path = os.path.join(os.path.dirname(project_root_abs), ".Jina-AI-Linker-Output", "ai_scores.json")
    if os.path.exists(os.path.join(project_root_abs, ".Jina-AI-Linker-Output")):
        ai_scores_file_path = os.path.join(project_root_abs, ".Jina-AI-Linker-Output", "ai_scores.json")
    
    existing_ai_scores = load_ai_scores_from_json(ai_scores_file_path)
    ai_score_cache.update(existing_ai_scores)  # é¢„å¡«å……ç¼“å­˜

    # ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰åµŒå…¥æ•°æ®
    embeddings_data_content = {}
    embeddings_file_path = os.path.join(project_root_abs, ".Jina-AI-Linker-Output", "jina_embeddings.json")
    if os.path.exists(embeddings_file_path):
        try:
            with open(embeddings_file_path, 'r', encoding='utf-8') as f:
                embeddings_data_content = json.load(f).get("files", {})
            print(f"âœ… æˆåŠŸåŠ è½½åµŒå…¥æ•°æ®ç”¨äºAIè¯„åˆ†ã€‚")
        except Exception as e:
            print(f"âš ï¸  è¯»å–åµŒå…¥æ•°æ®æ–‡ä»¶ {embeddings_file_path} å¤±è´¥: {e}ï¼Œå°†å›é€€åˆ°é€ä¸ªæ–‡ä»¶è¯»å–ã€‚")
            embeddings_data_content = {}
    
    # ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«å”¯ä¸€çš„å…³ç³»å¯¹ï¼ˆç”¨äºAIæ‰“åˆ†ï¼‰
    for pair in candidate_pairs_list:
        pair_id = pair.get("pair_id")
        if pair_id and pair_id not in unique_pairs_for_ai:
            # é€‰æ‹©å­—å…¸åºè¾ƒå°çš„ä½œä¸ºAIæ‰“åˆ†çš„"ä¸»"æ–¹å‘
            source_path = pair["source_path"]
            target_path = pair["target_path"]
            if source_path < target_path:
                unique_pairs_for_ai[pair_id] = pair
            # å¦‚æœå½“å‰pairçš„source > targetï¼Œç­‰å¾…åå‘pair
        elif pair_id and pair_id in unique_pairs_for_ai:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ä¸ºå­—å…¸åºæ›´å°çš„æ–¹å‘
            existing_pair = unique_pairs_for_ai[pair_id]
            if pair["source_path"] < existing_pair["source_path"]:
                unique_pairs_for_ai[pair_id] = pair
    
    print(f"å»é‡åéœ€è¦AIæ‰“åˆ†çš„å”¯ä¸€å…³ç³»å¯¹æ•°é‡: {len(unique_pairs_for_ai)} (åŸå§‹: {len(candidate_pairs_list)})")
    
    # ç¬¬äºŒæ­¥ï¼šå¯¹å”¯ä¸€çš„å…³ç³»å¯¹è¿›è¡ŒAIæ‰“åˆ†
    total_unique_pairs = len(unique_pairs_for_ai)
    processed_unique_pairs = 0
    
    for pair_id, pair in unique_pairs_for_ai.items():
        processed_unique_pairs += 1
        source_path = pair["source_path"]
        target_path = pair["target_path"]
        
        print(f"  AIæ‰“åˆ†å”¯ä¸€å¯¹ ({processed_unique_pairs}/{total_unique_pairs}): {source_path} <-> {target_path}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰AIè¯„åˆ†ï¼ˆå¦‚æœä¸æ˜¯å¼ºåˆ¶é‡æ–°è¯„åˆ†ï¼‰
        if not force_rescore and pair_id in ai_score_cache:
            print(f"    AIè¯„åˆ†å·²å­˜åœ¨äºç¼“å­˜ä¸­ (è¯„åˆ†: {ai_score_cache[pair_id]}/10)ï¼Œè·³è¿‡")
            continue
        
        # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡ŒAIæ‰“åˆ†
        source_abs_path = os.path.join(project_root_abs, source_path)
        target_abs_path = os.path.join(project_root_abs, target_path)
        
        if not os.path.exists(source_abs_path) or not os.path.exists(target_abs_path):
            print(f"    è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡AIæ‰“åˆ†")
            continue
            
        try:
            # å°è¯•ä»å·²åŠ è½½çš„åµŒå…¥æ•°æ®ä¸­è·å–å†…å®¹
            clean_source_body = embeddings_data_content.get(source_path, {}).get('processed_content')
            clean_target_body = embeddings_data_content.get(target_path, {}).get('processed_content')

            if not clean_source_body or not clean_target_body:
                # å¦‚æœæ— æ³•ä»åµŒå…¥æ•°æ®è·å–å†…å®¹ï¼Œå›é€€åˆ°æ–‡ä»¶è¯»å–
                print(f"    âš ï¸  æ— æ³•ä»ç¼“å­˜ä¸­è·å–å†…å®¹ï¼Œå°†ä»æ–‡ä»¶è¯»å–: {source_path} æˆ– {target_path}")
                source_body, _, _ = read_markdown_with_frontmatter(source_abs_path)
                target_body, _, _ = read_markdown_with_frontmatter(target_abs_path)
                
                clean_source_body = extract_content_for_hashing(source_body)
                clean_target_body = extract_content_for_hashing(target_body)
                
                if clean_source_body is None or clean_target_body is None:
                    print(f"    âŒ ç¼ºå°‘å“ˆå¸Œè¾¹ç•Œæ ‡è®°ï¼Œè·³è¿‡AIæ‰“åˆ†")
                    continue
                
            # æ‰§è¡ŒAIæ‰“åˆ†ï¼ˆåªè°ƒç”¨ä¸€æ¬¡APIï¼‰
            time.sleep(AI_API_REQUEST_DELAY_SECONDS)
            
            ai_result = call_ai_api_for_pair_relevance(
                clean_source_body,
                clean_target_body,
                source_path,
                target_path,
                ai_api_key,
                ai_provider,
                ai_api_url,
                ai_model_name,
                max_content_length_for_ai_to_use
            )
            
            if "ai_score" in ai_result and ai_result["ai_score"] != -1:
                ai_score = ai_result["ai_score"]
                ai_score_cache[pair_id] = ai_score
                print(f"    AIè¯„åˆ†æˆåŠŸ: {ai_score}/10")
            else:
                print(f"    AIè¯„åˆ†å¤±è´¥: {ai_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except Exception as e:
            print(f"    AIæ‰“åˆ†å¼‚å¸¸: {e}")
    
    # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜AIè¯„åˆ†ç»“æœåˆ°ç‹¬ç«‹JSONæ–‡ä»¶ï¼ˆä¸å†å†™å…¥frontmatterï¼‰
    print(f"\nä¿å­˜AIè¯„åˆ†ç»“æœåˆ°ç‹¬ç«‹JSONæ–‡ä»¶...")
    
    # ä¿å­˜AIè¯„åˆ†åˆ°ç‹¬ç«‹JSONæ–‡ä»¶
    save_ai_scores_to_json(ai_score_cache, unique_pairs_for_ai, ai_scores_file_path)
    
    print(f"\nAI æ‰“åˆ†å®Œæˆã€‚AIè¯„åˆ†æ•°æ®å·²ä¿å­˜åˆ°: {ai_scores_file_path}")
    print(f"æ³¨æ„ï¼šAIè¯„åˆ†ä¸å†å†™å…¥æ–‡ä»¶frontmatterï¼Œä»…ä¿å­˜åœ¨ç‹¬ç«‹çš„JSONæ–‡ä»¶ä¸­ã€‚")

def save_ai_scores_to_json(ai_score_cache: dict, unique_pairs_for_ai: dict, ai_scores_file_path: str):
    """
    ä¿å­˜AIè¯„åˆ†ç»“æœåˆ°ç‹¬ç«‹çš„JSONæ–‡ä»¶
    ä½¿ç”¨æ™ºèƒ½è·¯å¾„ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨æ–‡ä»¶åï¼Œå†²çªæ—¶ä½¿ç”¨å®Œæ•´è·¯å¾„
    """
    try:
        # åŠ è½½ç°æœ‰çš„AIè¯„åˆ†æ•°æ®
        existing_ai_scores = {}
        if os.path.exists(ai_scores_file_path):
            try:
                with open(ai_scores_file_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_ai_scores = existing_data.get("ai_scores", {})
            except Exception as e:
                print(f"è­¦å‘Šï¼šè¯»å–ç°æœ‰AIè¯„åˆ†æ–‡ä»¶å¤±è´¥: {e}")
        
        # ä½¿ç”¨å®Œæ•´è·¯å¾„å­˜å‚¨ï¼Œæ— éœ€å†²çªæ£€æµ‹
        
        def get_smart_key(path1: str, path2: str) -> str:
            """ç”Ÿæˆæ ‡å‡†åŒ–çš„é”®åï¼šä½¿ç”¨å®Œæ•´ç›¸å¯¹è·¯å¾„ï¼ŒæŒ‰å­—å…¸åºæ’åº"""
            # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
            norm_path1 = path1.replace(os.sep, '/')
            norm_path2 = path2.replace(os.sep, '/')
            
            # æŒ‰å­—å…¸åºæ’åºï¼Œç¡®ä¿ä¸€è‡´æ€§
            return f"{min(norm_path1, norm_path2)}<->{max(norm_path1, norm_path2)}"
        
        # æ›´æ–°AIè¯„åˆ†æ•°æ®
        updated_count = 0
        for pair_id, ai_score in ai_score_cache.items():
            if pair_id in unique_pairs_for_ai:
                pair_info = unique_pairs_for_ai[pair_id]
                source_path = pair_info["source_path"]
                target_path = pair_info["target_path"]
                
                # ç”Ÿæˆæ™ºèƒ½é”®å
                smart_key = get_smart_key(source_path, target_path)
                
                # åˆ›å»ºè¯„åˆ†æ¡ç›®
                score_entry = {
                    "ai_score": ai_score,
                    "jina_similarity": round(pair_info["jina_similarity"], 6),
                    "last_scored": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                    "source_path": source_path,
                    "target_path": target_path,
                    "key_type": "full_path"
                }
                
                existing_ai_scores[smart_key] = score_entry
                updated_count += 1
        
        # æ„å»ºæœ€ç»ˆæ•°æ®ç»“æ„
        final_data = {
            "_metadata": {
                "version": "1.0",
                "description": "AIè¯„åˆ†æ•°æ® - å®Œæ•´è·¯å¾„å­˜å‚¨",
                "last_updated": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                "total_relationships": len(existing_ai_scores),
                "storage_strategy": "full_path"
            },
            "ai_scores": existing_ai_scores
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(os.path.dirname(ai_scores_file_path), exist_ok=True)
        with open(ai_scores_file_path, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… AIè¯„åˆ†æ•°æ®å·²ä¿å­˜: {updated_count} ä¸ªå…³ç³» (ä½¿ç”¨å®Œæ•´è·¯å¾„å­˜å‚¨)")
            
    except Exception as e:
        print(f"âŒ ä¿å­˜AIè¯„åˆ†æ•°æ®å¤±è´¥: {e}")

def load_ai_scores_from_json(ai_scores_file_path: str) -> dict:
    """
    ä»JSONæ–‡ä»¶åŠ è½½AIè¯„åˆ†æ•°æ®
    è¿”å› {pair_id: ai_score} æ ¼å¼çš„å­—å…¸
    """
    ai_scores = {}
    if not os.path.exists(ai_scores_file_path):
        return ai_scores
    
    try:
        with open(ai_scores_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            stored_scores = data.get("ai_scores", {})
            
            for key, entry in stored_scores.items():
                if isinstance(entry, dict) and "ai_score" in entry:
                    # å°†å­˜å‚¨çš„é”®è½¬æ¢å›pair_idæ ¼å¼
                    source_path = entry.get("source_path", "")
                    target_path = entry.get("target_path", "")
                    
                    if source_path and target_path:
                        # ç”Ÿæˆæ ‡å‡†çš„pair_id
                        pair_id = f"{min(source_path, target_path)}<->{max(source_path, target_path)}"
                        ai_scores[pair_id] = entry["ai_score"]
        
        print(f"ğŸ“– ä»AIè¯„åˆ†æ–‡ä»¶åŠ è½½äº† {len(ai_scores)} ä¸ªè¯„åˆ†è®°å½•")
        
    except Exception as e:
        print(f"âš ï¸ åŠ è½½AIè¯„åˆ†æ–‡ä»¶å¤±è´¥: {e}")
    
    return ai_scores

# å·²åˆ é™¤ build_file_index å‡½æ•°ï¼Œå› ä¸ºYAMLè·¯å¾„æ›´æ–°åŠŸèƒ½å·²è¢«ç§»é™¤

def update_target_paths_in_frontmatter_for_single_file(
    file_abs_path: str, 
    file_index: dict, 
    unfound_targets: set
) -> bool:
    """
    æ­¤å‡½æ•°å·²åˆ é™¤ï¼Œå› ä¸ºAIè¯„åˆ†ä¸å†å†™å…¥YAML frontmatter
    """
    return False

def update_all_target_paths_in_vault(
    project_root_abs: str,
    excluded_folders: list = None,
    excluded_files_patterns: list = None
):
    """
    æ­¤å‡½æ•°å·²åˆ é™¤ï¼Œå› ä¸ºAIè¯„åˆ†ä¸å†å†™å…¥YAML frontmatter
    """
    print(f"\n===== YAML è·¯å¾„æ›´æ–°åŠŸèƒ½å·²åˆ é™¤ =====")
    print(f"ç”±äºAIè¯„åˆ†ä¸å†å†™å…¥YAML frontmatterï¼Œæ­¤åŠŸèƒ½å·²è¢«åˆ é™¤ã€‚")


# --- Default constants for argparse ---
DEFAULT_EMBEDDINGS_FILE_NAME = "jina_embeddings.json"
DEFAULT_CANDIDATES_FILE_NAME = "jina_candidate_pairs.json"
DEFAULT_SIMILARITY_THRESHOLD = 0.70

def main():
    print("ğŸš€ Jina AI å¤„ç†å·¥å…·å¯åŠ¨")
    parser = argparse.ArgumentParser(description="Jina AI å¤„ç†å·¥å…· - å¤„ç†ç¬”è®°å†…å®¹å¹¶æå–åµŒå…¥ã€‚")
    parser.add_argument('--project_root', type=str, required=True, help='é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='.Jina-AI-Linker-Output', help='è¾“å‡ºæ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰')
    parser.add_argument('--jina_api_key', type=str, default='', help='Jina API å¯†é’¥') # Made optional for path update mode
    # AI æä¾›å•†å‚æ•°
    parser.add_argument('--ai_provider', type=str, default='', help='AI æä¾›å•† (deepseek, openai, claude, gemini, custom)')
    parser.add_argument('--ai_api_url', type=str, default='', help='AI API URL')
    parser.add_argument('--ai_api_key', type=str, default='', help='AI API å¯†é’¥ï¼ˆç”¨äº AI æ‰“åˆ†ï¼Œå¦‚ä¸æä¾›åˆ™è·³è¿‡ AI æ‰“åˆ†ï¼‰')
    parser.add_argument('--ai_model_name', type=str, default='', help='AI æ¨¡å‹åç§°')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--similarity_threshold', type=float, default=0.7, help='ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰')
    parser.add_argument('--scan_target_folders', nargs='*', default=[], help='è¦æ‰«æçš„æ–‡ä»¶å¤¹ï¼ˆé€—å·åˆ†éš”ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰')
    parser.add_argument('--excluded_folders', nargs='*', default=[], help='è¦æ’é™¤çš„æ–‡ä»¶å¤¹åˆ—è¡¨')
    parser.add_argument('--excluded_files_patterns', nargs='*', default=[], help='è¦æ’é™¤çš„æ–‡ä»¶åæ¨¡å¼åˆ—è¡¨')
    parser.add_argument('--jina_model_name', type=str, default='jina-embeddings-v3', help='Jina æ¨¡å‹åç§°')
    parser.add_argument('--max_chars_for_jina', type=int, default=8000, help='ä¼ é€’ç»™ Jina çš„æœ€å¤§å­—ç¬¦æ•°')
    parser.add_argument('--max_content_length_for_ai', type=int, default=5000, help='ä¼ é€’ç»™ AI è¯„åˆ†çš„æ¯ç¯‡ç¬”è®°çš„æœ€å¤§å†…å®¹é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰')
    parser.add_argument('--max_candidates_per_source_for_ai_scoring', type=int, default=20, help='æ¯ä¸ªæºç¬”è®°å‘é€ç»™ AI è¯„åˆ†çš„æœ€å¤§å€™é€‰é“¾æ¥æ•°')
    parser.add_argument('--ai_scoring_mode', type=str, choices=['force', 'smart', 'skip'], default='smart', help='AI è¯„åˆ†æ¨¡å¼ï¼šforce=å¼ºåˆ¶é‡æ–°è¯„åˆ†æ‰€æœ‰å€™é€‰ï¼Œsmart=åªè¯„åˆ†æœªè¯„åˆ†çš„ï¼Œskip=è·³è¿‡ AI è¯„åˆ†')
    parser.add_argument('--hash_boundary_marker', type=str, default='<!-- HASH_BOUNDARY -->', help='ç”¨äºæ ‡è®°å“ˆå¸Œè®¡ç®—è¾¹ç•Œçš„æ ‡è®°')
    parser.add_argument('--update_paths_only', action='store_true', help='åªæ‰§è¡Œ YAML è·¯å¾„æ›´æ–°åŠŸèƒ½ï¼Œä¸æ‰§è¡Œå…¶ä»–å¤„ç†ã€‚') # New argument
    
    args = parser.parse_args()
    
    start_time = time.time()

    project_root_abs = os.path.abspath(args.project_root)
    output_dir_in_vault = args.output_dir
    output_dir_abs = os.path.join(project_root_abs, output_dir_in_vault)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir_abs, exist_ok=True)
    
    # If only updating paths, execute that function and exit
    if args.update_paths_only:
        update_all_target_paths_in_vault(
            project_root_abs,
            excluded_folders=args.excluded_folders,
            excluded_files_patterns=args.excluded_files_patterns
        )
        end_time = time.time()
        print(f"\næ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        return

    # Default processing flow continues below if not update_paths_only
    
    # é»˜è®¤çš„åµŒå…¥å’Œå€™é€‰æ–‡ä»¶è·¯å¾„
    embeddings_file_path = os.path.join(output_dir_abs, "jina_embeddings.json")
    
    # å¤„ç†æ‰«æç›®æ ‡æ–‡ä»¶å¤¹å‚æ•°
    if not args.scan_target_folders or (len(args.scan_target_folders) == 1 and args.scan_target_folders[0] == '/'):
        scan_target_folder_abs = project_root_abs
        scan_target_folder_rel = "/"
    else:
        scan_targets = args.scan_target_folders
        if len(scan_targets) == 1:
            scan_target_folder_rel = scan_targets[0]
            scan_target_folder_abs = os.path.join(project_root_abs, scan_target_folder_rel)
            scan_target_folder_rel = scan_target_folder_rel.replace(os.sep, '/')
        else:
            # å½“å­˜åœ¨å¤šä¸ªæ‰«æç›®æ ‡æ—¶ï¼Œå…ˆæ‰«ææ•´ä¸ªåº“ï¼Œåé¢ä¼šè¿‡æ»¤åªå¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
            scan_target_folder_abs = project_root_abs
            scan_target_folder_rel = "multiple folders"
    
    print(f"===== Jinaå¤„ç†å¯åŠ¨ =====")
    print(f"ğŸ“‚ æ‰«æç›®æ ‡: {scan_target_folder_rel}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir_in_vault}")
    print(f"ğŸ¤– Jinaæ¨¡å‹: {args.jina_model_name}")
    print(f"ğŸ¯ ç›¸ä¼¼åº¦é˜ˆå€¼: {args.similarity_threshold}")
    if args.max_candidates_per_source_for_ai_scoring > 0:
        print(f"- æ¯æºç¬”è®°çš„æœ€å¤§AIè¯„åˆ†å€™é€‰æ•°: {args.max_candidates_per_source_for_ai_scoring}")
    if args.ai_api_key:
        print(f"- AIè¯„åˆ†æ¨¡å¼: {args.ai_scoring_mode}")
        print(f"- AIæä¾›å•†: {args.ai_provider}")
        print(f"- AIæ¨¡å‹: {args.ai_model_name}")
        print(f"- AIè¯„åˆ†å†…å®¹æœ€å¤§é•¿åº¦: {args.max_content_length_for_ai}")
    else:
        print("- AIè¯„åˆ†: æœªæä¾› AI API å¯†é’¥ï¼Œè·³è¿‡ AI è¯„åˆ†")
    
    # æ‰«æå¹¶åˆ—å‡ºç¬¦åˆæ¡ä»¶çš„ markdown æ–‡ä»¶
    print(f"\nğŸ“ æ­¥éª¤ 1ï¼šæ‰«æ Markdown æ–‡ä»¶...")
    if scan_target_folder_rel == "multiple folders":
        # å¦‚æœæŒ‡å®šäº†å¤šä¸ªæ‰«ææ–‡ä»¶å¤¹ï¼Œå…ˆæ‰«æå…¨éƒ¨
        all_markdown_files = list_markdown_files(
            project_root_abs, 
            project_root_abs,
            excluded_folders=args.excluded_folders,
            excluded_files_patterns=args.excluded_files_patterns
        )
        # ç„¶ååªä¿ç•™æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶
        target_folders = [folder.replace(os.sep, '/') for folder in args.scan_target_folders]
        filtered_markdown_files = []
        for file_path in all_markdown_files:
            file_path_norm = normalize_path_python(file_path)
            for target_folder in target_folders:
                target_folder_norm = normalize_path_python(target_folder)
                if target_folder_norm == "/":  # æ ¹ç›®å½•ç‰¹æ®Šå¤„ç†
                    if "/" not in file_path_norm:
                        filtered_markdown_files.append(file_path)
                        break
                elif file_path_norm == target_folder_norm or file_path_norm.startswith(target_folder_norm + "/"):
                    filtered_markdown_files.append(file_path)
                    break
        markdown_files_to_process = filtered_markdown_files
    else:
        # æ­£å¸¸å¤„ç†å•ä¸ªç›®æ ‡æ–‡ä»¶å¤¹çš„æƒ…å†µ
        markdown_files_to_process = list_markdown_files(
            scan_target_folder_abs, 
            project_root_abs,
            excluded_folders=args.excluded_folders,
            excluded_files_patterns=args.excluded_files_patterns
        )

    if not markdown_files_to_process:
        print("  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ Markdown æ–‡ä»¶ï¼")
        return
    
    print(f"  æ‰¾åˆ° {len(markdown_files_to_process)} ä¸ª Markdown æ–‡ä»¶ã€‚")
    
    # æ­¥éª¤2ï¼šä½¿ç”¨ Jina AI å¤„ç†ç¬”è®°å¹¶ç”ŸæˆåµŒå…¥
    print(f"\nğŸ§  æ­¥éª¤ 2ï¼šå¤„ç†ç¬”è®°å¹¶ç”ŸæˆåµŒå…¥...")
    embeddings_data = process_and_embed_notes(
        project_root_abs,
        markdown_files_to_process,
        embeddings_file_path,
        jina_api_key_to_use=args.jina_api_key,
        jina_model_name_to_use=args.jina_model_name,
        max_chars_for_jina_to_use=args.max_chars_for_jina
    )
    
    if not embeddings_data or not embeddings_data.get('files'):
        print("  é”™è¯¯ï¼šæ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶æˆ–ç”ŸæˆåµŒå…¥ã€‚")
        return
    
    # æ­¥éª¤3ï¼šç”Ÿæˆå€™é€‰é“¾æ¥å¯¹
    print(f"\nğŸ”— æ­¥éª¤ 3ï¼šæ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼ {args.similarity_threshold} ç”Ÿæˆå€™é€‰é“¾æ¥å¯¹...")
    candidate_pairs = generate_candidate_pairs(embeddings_data, args.similarity_threshold)
    
    print(f"  å…±ç”Ÿæˆ {len(candidate_pairs)} ä¸ªå€™é€‰é“¾æ¥å¯¹ã€‚")
    if len(candidate_pairs) == 0:
        print("  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆç›¸ä¼¼åº¦é˜ˆå€¼çš„å€™é€‰é“¾æ¥å¯¹ã€‚")
    
    # æ­¥éª¤ 4: AI å¯¹å€™é€‰é“¾æ¥è¿›è¡Œæ™ºèƒ½æ‰“åˆ†è¯„åˆ†
    if (args.ai_api_key and args.ai_scoring_mode != 'skip' and 
        args.max_candidates_per_source_for_ai_scoring > 0 and len(candidate_pairs) > 0):
        
        ai_provider_name = {
            'deepseek': 'DeepSeek',
            'openai': 'OpenAI', 
            'claude': 'Claude',
            'gemini': 'Gemini',
            'custom': 'è‡ªå®šä¹‰AI'
        }.get(args.ai_provider, args.ai_provider)
        
        scoring_mode_text = {
            'force': 'å¼ºåˆ¶é‡æ–°è¯„åˆ†',
            'smart': 'æ™ºèƒ½è¯„åˆ†',
            'skip': 'è·³è¿‡è¯„åˆ†'
        }.get(args.ai_scoring_mode, 'æ™ºèƒ½è¯„åˆ†')
        
        print(f"\nğŸ¤– æ­¥éª¤ 4ï¼šä½¿ç”¨ {ai_provider_name} AI å¯¹å€™é€‰é“¾æ¥è¿›è¡Œ{scoring_mode_text}...")
        
        force_rescore = args.ai_scoring_mode == 'force'
        score_candidates_and_update_frontmatter(
            candidate_pairs,
            project_root_abs,
            ai_provider=args.ai_provider,
            ai_api_url=args.ai_api_url,
            ai_api_key=args.ai_api_key,
            ai_model_name=args.ai_model_name,
            max_content_length_for_ai_to_use=args.max_content_length_for_ai,
            max_candidates_per_source_for_ai_scoring_to_use=args.max_candidates_per_source_for_ai_scoring, 
            hash_boundary_marker_to_use=args.hash_boundary_marker,
            force_rescore=force_rescore
        )
    else:
        if args.ai_api_key:
            print(f"\nâ­ï¸ æ­¥éª¤ 4ï¼šè·³è¿‡ AI è¯„åˆ† (è¯„åˆ†æ¨¡å¼: {args.ai_scoring_mode})")
        else:
            print(f"\nâ­ï¸ æ­¥éª¤ 4ï¼šè·³è¿‡ AI è¯„åˆ† (æœªæä¾› {args.ai_provider or 'AI'} API å¯†é’¥)")
    
    # æ‰“å°æ€»ç»“ä¿¡æ¯
    end_time = time.time()
    total_files_processed = len(embeddings_data.get('files', {}))
    total_time = end_time - start_time
    
    print(f"\nâœ… ===== å¤„ç†å®Œæˆ =====")
    print(f"ğŸ“Š æˆåŠŸå¤„ç†æ–‡ä»¶: {total_files_processed} ä¸ª")
    print(f"ğŸ”— ç”Ÿæˆå€™é€‰é“¾æ¥å¯¹: {len(candidate_pairs)} ä¸ª")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ’¾ åµŒå…¥æ•°æ®ä¿å­˜è‡³: {embeddings_file_path}")

if __name__ == "__main__":
    main()
