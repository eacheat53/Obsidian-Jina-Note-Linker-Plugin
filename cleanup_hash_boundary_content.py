#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡åˆ é™¤Markdownæ–‡ä»¶ä¸­ <!-- HASH_BOUNDARY --> ä¹‹åçš„æ‰€æœ‰å†…å®¹
ä¿ç•™YAML frontmatterå’Œå“ˆå¸Œè¾¹ç•Œæ ‡è®°ä¹‹å‰çš„æ­£æ–‡å†…å®¹
"""

import os
import re
import argparse
from pathlib import Path
import shutil
from datetime import datetime

# å“ˆå¸Œè¾¹ç•Œæ ‡è®°å¸¸é‡
HASH_BOUNDARY_MARKER = "<!-- HASH_BOUNDARY -->"

def process_markdown_file(file_path, backup_dir=None, dry_run=False):
    """
    å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶ï¼Œåˆ é™¤å“ˆå¸Œè¾¹ç•Œæ ‡è®°ä¹‹åçš„æ‰€æœ‰å†…å®¹
    """
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å“ˆå¸Œè¾¹ç•Œæ ‡è®°
        if HASH_BOUNDARY_MARKER not in content:
            return False, "æœªæ‰¾åˆ°å“ˆå¸Œè¾¹ç•Œæ ‡è®°"
        
        # æ‰¾åˆ°å“ˆå¸Œè¾¹ç•Œæ ‡è®°çš„ä½ç½®
        boundary_index = content.find(HASH_BOUNDARY_MARKER)
        
        # æå–å“ˆå¸Œè¾¹ç•Œæ ‡è®°ä¹‹å‰çš„å†…å®¹ï¼ˆåŒ…æ‹¬æ ‡è®°æœ¬èº«ï¼‰
        content_before_boundary = content[:boundary_index + len(HASH_BOUNDARY_MARKER)]
        content_after_boundary = content[boundary_index + len(HASH_BOUNDARY_MARKER):]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹éœ€è¦åˆ é™¤
        if not content_after_boundary.strip():
            return False, "å“ˆå¸Œè¾¹ç•Œæ ‡è®°ä¹‹åæ— å†…å®¹"
        
        # ç»Ÿè®¡è¦åˆ é™¤çš„å†…å®¹
        lines_after = content_after_boundary.strip().split('\n')
        deleted_lines = len(lines_after)
        
        # é¢„è§ˆè¦åˆ é™¤çš„å†…å®¹
        preview_content = content_after_boundary.strip()
        if len(preview_content) > 200:
            preview_content = preview_content[:200] + "..."
        
        if dry_run:
            return True, f"å°†åˆ é™¤ {deleted_lines} è¡Œå†…å®¹: {repr(preview_content)}"
        
        # åˆ›å»ºå¤‡ä»½
        if backup_dir:
            backup_path = backup_dir / file_path.name
            shutil.copy2(file_path, backup_path)
        
        # å†™å…¥å¤„ç†åçš„å†…å®¹
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content_before_boundary)
        
        return True, f"å·²åˆ é™¤ {deleted_lines} è¡Œå†…å®¹"
        
    except Exception as e:
        return False, f"å¤„ç†å¤±è´¥: {e}"

def find_markdown_files(directory, exclude_patterns=None):
    """
    é€’å½’æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    """
    if exclude_patterns is None:
        exclude_patterns = ['.obsidian', '.git', 'node_modules']
    
    markdown_files = []
    for root, dirs, files in os.walk(directory):
        # æ’é™¤æŒ‡å®šç›®å½•
        dirs[:] = [d for d in dirs if not any(pattern in d for pattern in exclude_patterns)]
        
        for file in files:
            if file.endswith('.md'):
                markdown_files.append(Path(root) / file)
    
    return markdown_files

def analyze_file_content(file_path):
    """
    åˆ†ææ–‡ä»¶å†…å®¹ï¼Œè¿”å›å“ˆå¸Œè¾¹ç•Œæ ‡è®°å‰åçš„å†…å®¹ä¿¡æ¯
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if HASH_BOUNDARY_MARKER not in content:
            return None, "æœªæ‰¾åˆ°å“ˆå¸Œè¾¹ç•Œæ ‡è®°"
        
        boundary_index = content.find(HASH_BOUNDARY_MARKER)
        content_before = content[:boundary_index]
        content_after = content[boundary_index + len(HASH_BOUNDARY_MARKER):]
        
        # åˆ†æå†…å®¹
        lines_before = len(content_before.split('\n'))
        lines_after = len(content_after.strip().split('\n')) if content_after.strip() else 0
        
        # åˆ†æåˆ é™¤å†…å®¹çš„ç±»å‹
        after_content = content_after.strip()
        content_types = []
        
        if '## å»ºè®®é“¾æ¥' in after_content:
            content_types.append('å»ºè®®é“¾æ¥')
        if '<!-- LINKS_START -->' in after_content:
            content_types.append('é“¾æ¥æ ‡è®°')
        if after_content.count('[[') > 0:
            link_count = after_content.count('[[')
            content_types.append(f'{link_count}ä¸ªé“¾æ¥')
        
        content_type_str = ', '.join(content_types) if content_types else 'å…¶ä»–å†…å®¹'
        
        return {
            'lines_before': lines_before,
            'lines_after': lines_after,
            'content_types': content_type_str,
            'preview': after_content[:100] + "..." if len(after_content) > 100 else after_content
        }, None
        
    except Exception as e:
        return None, f"åˆ†æå¤±è´¥: {e}"

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡åˆ é™¤Markdownæ–‡ä»¶ä¸­å“ˆå¸Œè¾¹ç•Œæ ‡è®°ä¹‹åçš„æ‰€æœ‰å†…å®¹')
    parser.add_argument('directory', help='è¦å¤„ç†çš„ç›®å½•è·¯å¾„')
    parser.add_argument('--backup', action='store_true', help='åˆ›å»ºå¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶')
    parser.add_argument('--exclude', 
                       default='.obsidian,.git,node_modules',
                       help='è¦æ’é™¤çš„ç›®å½•æ¨¡å¼ï¼Œç”¨é€—å·åˆ†éš”')
    parser.add_argument('--analyze', action='store_true', help='åˆ†ææ¨¡å¼ï¼Œè¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„å†…å®¹ç»“æ„')
    
    args = parser.parse_args()
    
    # è§£æå‚æ•°
    directory = Path(args.directory)
    exclude_patterns = [pattern.strip() for pattern in args.exclude.split(',')]
    
    if not directory.exists():
        print(f"é”™è¯¯ï¼šç›®å½• {directory} ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    backup_dir = None
    if args.backup and not args.dry_run:
        backup_dir = directory / f"hash_boundary_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        backup_dir.mkdir(exist_ok=True)
        print(f"å¤‡ä»½ç›®å½•: {backup_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    markdown_files = find_markdown_files(directory, exclude_patterns)
    
    if not markdown_files:
        print("æœªæ‰¾åˆ°ä»»ä½•Markdownæ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
    print(f"å“ˆå¸Œè¾¹ç•Œæ ‡è®°: {HASH_BOUNDARY_MARKER}")
    print(f"æ’é™¤ç›®å½•: {', '.join(exclude_patterns)}")
    
    if args.dry_run:
        print("\n=== é¢„è§ˆæ¨¡å¼ ===")
    elif args.analyze:
        print("\n=== åˆ†ææ¨¡å¼ ===")
    else:
        print("\n=== å¼€å§‹å¤„ç† ===")
    
    processed_count = 0
    modified_count = 0
    total_deleted_lines = 0
    
    for i, file_path in enumerate(markdown_files, 1):
        relative_path = file_path.relative_to(directory)
        print(f"\n[{i}/{len(markdown_files)}] å¤„ç†: {relative_path}")
        
        if args.analyze:
            # åˆ†ææ¨¡å¼
            analysis, error = analyze_file_content(file_path)
            if error:
                print(f"  âŒ {error}")
                continue
            
            print(f"  ğŸ“Š è¾¹ç•Œå‰: {analysis['lines_before']} è¡Œ")
            print(f"  ğŸ“Š è¾¹ç•Œå: {analysis['lines_after']} è¡Œ")
            print(f"  ğŸ“‹ å†…å®¹ç±»å‹: {analysis['content_types']}")
            print(f"  ğŸ‘€ é¢„è§ˆ: {repr(analysis['preview'])}")
            
            if analysis['lines_after'] > 0:
                processed_count += 1
                total_deleted_lines += analysis['lines_after']
        else:
            # å¤„ç†æ¨¡å¼
            success, message = process_markdown_file(file_path, backup_dir, args.dry_run)
            
            if success:
                processed_count += 1
                if not args.dry_run:
                    modified_count += 1
                print(f"  âœ… {message}")
                
                # ç»Ÿè®¡åˆ é™¤çš„è¡Œæ•°
                if "åˆ é™¤" in message and "è¡Œ" in message:
                    try:
                        lines = int(message.split("åˆ é™¤")[1].split("è¡Œ")[0].strip())
                        total_deleted_lines += lines
                    except:
                        pass
            else:
                print(f"  â­ï¸  è·³è¿‡: {message}")
    
    # è¾“å‡ºæ€»ç»“
    print(f"\n=== å¤„ç†å®Œæˆ ===")
    print(f"æ€»æ–‡ä»¶æ•°: {len(markdown_files)}")
    print(f"åŒ…å«å“ˆå¸Œè¾¹ç•Œæ ‡è®°çš„æ–‡ä»¶: {processed_count}")
    
    if args.analyze:
        print(f"æ€»è®¡å°†åˆ é™¤è¡Œæ•°: {total_deleted_lines}")
        print("åˆ†æå®Œæˆã€‚è¦å®é™…æ‰§è¡Œåˆ é™¤ï¼Œè¯·ç§»é™¤ --analyze å‚æ•°")
    elif args.dry_run:
        print(f"é¢„è®¡åˆ é™¤è¡Œæ•°: {total_deleted_lines}")
        print("é¢„è§ˆæ¨¡å¼ï¼šæœªå®é™…ä¿®æ”¹ä»»ä½•æ–‡ä»¶")
        print("è¦å®é™…æ‰§è¡Œä¿®æ”¹ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
    else:
        print(f"å®é™…ä¿®æ”¹çš„æ–‡ä»¶: {modified_count}")
        print(f"æ€»è®¡åˆ é™¤è¡Œæ•°: {total_deleted_lines}")
        if backup_dir:
            print(f"å¤‡ä»½ä½ç½®: {backup_dir}")

if __name__ == "__main__":
    main()